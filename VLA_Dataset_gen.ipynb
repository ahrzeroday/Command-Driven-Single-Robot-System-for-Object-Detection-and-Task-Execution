{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8f5117",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec7fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395e096",
   "metadata": {},
   "source": [
    "## 2. Configuration for Dataset Generation\n",
    "\n",
    "This block defines the key parameters and constants for the data generation process. It specifies where the dataset will be saved, how much data to create, and the different text prompts to be associated with the robot's actions.\n",
    "\n",
    "* **`DATASET_PATH`** and **`DATASET_NAME`**: These strings define the directory structure for saving the generated dataset.\n",
    "* **`NUM_EPISODES`**: This sets the total number of simulation runs to be recorded. Each episode will represent one complete attempt at the task.\n",
    "* **`VISUAL_CONFIRMATION`**: A boolean flag that controls how the simulation is run. If `True`, the simulation will run with a graphical user interface (GUI) so you can watch the robot. If `False`, it will run in headless mode (`p.DIRECT`), which is much faster and ideal for generating large amounts of data.\n",
    "* **`PROMPTS`**: This is a list of different natural language commands that all describe the same task. Providing this **linguistic diversity** is crucial for training a robust model that can understand various ways a user might phrase a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383aa553",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"datasets\" \n",
    "DATASET_NAME = \"my_robot_dataset\"\n",
    "NUM_EPISODES = 100\n",
    "VISUAL_CONFIRMATION = True\n",
    "\n",
    "PROMPTS = [\n",
    "    \"place the cube in the tray\", \"put the block into the container\", \"move the cube to the tray\",\n",
    "    \"can you drop the cube inside the tray?\", \"pick up the cube and put it in the tray\",\n",
    "    \"grasp the block and place it in the tray\", \"transfer the cube to the tray\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773e844",
   "metadata": {},
   "source": [
    "## 3. Dataset Generation Pipeline\n",
    "\n",
    "This part defines a complete pipeline for generating a robotics dataset. It perform a pick-and-place task in a PyBullet simulation and saves the resulting trajectories (observations, actions, instructions) in the standardized `TFDS` (TensorFlow Datasets) format.\n",
    "\n",
    "### 3.1. The Expert Policy (`get_expert_waypoints`)\n",
    "This function acts as a perfect, hard-coded agent or \"expert.\" It doesn't use AI; instead, it generates a deterministic sequence of waypoints (key positions and gripper states) to flawlessly solve the task. This scripted demonstration provides the ground-truth actions for our dataset. The sequence is logical:\n",
    "1.  Open gripper.\n",
    "2.  Move above the cube.\n",
    "3.  Descend to grasp the cube.\n",
    "4.  Close gripper.\n",
    "5.  Lift the cube.\n",
    "6.  Move above the tray.\n",
    "7.  Descend to place the cube.\n",
    "8.  Open gripper.\n",
    "9.  Retract upwards.\n",
    "\n",
    "### 3.2. The Episode Generator (`generate_episode_for_rlds`)\n",
    "This is the main workhorse function that runs a single simulation episode and collects the data.\n",
    "* **Smart Simulation Mode**: It cleverly runs the *first* episode in GUI mode for a quick visual check, then switches to the much faster headless (`DIRECT`) mode for the rest of the data generation.\n",
    "* **Randomized Setup**: It creates a randomized scene for each episode (cube/tray positions and cube color) to ensure the dataset is diverse.\n",
    "* **Data Recording Loop**: It follows the expert's waypoints. At each step, it records a dictionary containing:\n",
    "    * **`observation`**\n",
    "    * **`action`**: The action taken to reach the next state (change in position and gripper command).\n",
    "    * **`language_instruction`**: A randomly chosen text prompt.\n",
    "    * **Metadata**: Flags like `is_first` and `is_last` which are standard in RL datasets.\n",
    "\n",
    "### 3.3. The TensorFlow Dataset Class (`MyRobotDataset`)\n",
    "This class integrates our custom data generation logic with the `TFDS` framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d42389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_waypoints(robot_id, cube_id, tray_id):\n",
    "    \"\"\"\n",
    "    Generates a hard-coded sequence of waypoints for a perfect pick-and-place demonstration.\n",
    "    This acts as the \"expert\" policy.\n",
    "    \"\"\"\n",
    "    waypoints = []\n",
    "    # Get the current positions of the key objects.\n",
    "    cube_pos, _ = p.getBasePositionAndOrientation(cube_id)\n",
    "    tray_pos, _ = p.getBasePositionAndOrientation(tray_id)\n",
    "    robot_pos, _ = p.getBasePositionAndOrientation(robot_id)\n",
    "    dx, dy = cube_pos[0] - robot_pos[0], cube_pos[1] - robot_pos[1]\n",
    "    yaw_angle = np.pi / 2 if abs(dy) > abs(dx) else 0\n",
    "\n",
    "    # Define a fixed orientation for the gripper during the task.\n",
    "    target_orn = p.getQuaternionFromEuler([np.pi, 0, yaw_angle])\n",
    "\n",
    "    # Define the precise position for grasping the cube.\n",
    "    grasp_pos = np.array([cube_pos[0], cube_pos[1], cube_pos[2] - 0.015])\n",
    "\n",
    "    # Define the sequence of movements and gripper actions.\n",
    "    waypoints.extend([\n",
    "        {'type': 'gripper', 'cmd': 0.0},  # 1. Start with gripper open.\n",
    "        {'type': 'move', 'pos': np.array([cube_pos[0], cube_pos[1], cube_pos[2] + 0.2]), 'orn': target_orn}, # 2. Move above the cube.\n",
    "        {'type': 'move', 'pos': grasp_pos, 'orn': target_orn}, # 3. Lower to grasp position.\n",
    "        {'type': 'gripper', 'cmd': 1.0},  # 4. Close gripper.\n",
    "        {'type': 'move', 'pos': np.array([cube_pos[0], cube_pos[1], cube_pos[2] + 0.2]), 'orn': target_orn}, # 5. Lift the cube up.\n",
    "        {'type': 'move', 'pos': np.array([tray_pos[0], tray_pos[1], tray_pos[2] + 0.2]), 'orn': target_orn}, # 6. Move above the tray.\n",
    "        {'type': 'move', 'pos': np.array([tray_pos[0], tray_pos[1], tray_pos[2] + 0.04]), 'orn': target_orn}, # 7. Lower to place position.\n",
    "        {'type': 'gripper', 'cmd': 0.0},  # 8. Open gripper to release.\n",
    "        {'type': 'move', 'pos': np.array([tray_pos[0], tray_pos[1], tray_pos[2] + 0.2]), 'orn': target_orn}  # 9. Retract upwards.\n",
    "    ])\n",
    "    return waypoints\n",
    "\n",
    "def generate_episode_for_rlds(episode_idx):\n",
    "    \"\"\"\n",
    "    Runs a full simulation episode, executes the expert policy, and records all data.\n",
    "    \"\"\"\n",
    "    # Use GUI for the first episode for a visual check, then headless for speed.\n",
    "    connection_mode = p.GUI if VISUAL_CONFIRMATION and episode_idx == 0 else p.DIRECT\n",
    "    \n",
    "    p.connect(connection_mode)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath()); p.setGravity(0, 0, -9.81)\n",
    "\n",
    "    # If in GUI mode, configure the visualizer.\n",
    "    if connection_mode == p.GUI:\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_SHADOWS, 1)\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_RENDERING, 1)\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_RGB_BUFFER_PREVIEW, 0)\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_DEPTH_BUFFER_PREVIEW, 0)\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_SEGMENTATION_MARK_PREVIEW, 0)\n",
    "        p.resetDebugVisualizerCamera(cameraDistance=0.85,\n",
    "                                cameraYaw=50,\n",
    "                                cameraPitch=-35,\n",
    "                                cameraTargetPosition=[0, 0, 0.7])\n",
    "        time.sleep(1)\n",
    "\n",
    "    p.loadURDF(\"plane.urdf\")\n",
    "    p.loadURDF(\"table/table.urdf\", basePosition=[0, 0, 0])\n",
    "    robot_id = p.loadURDF(\"franka_panda/panda.urdf\", basePosition=[-0.45, 0, 0.625], useFixedBase=True)\n",
    "    # Reset robot to a default pose.\n",
    "    for i, angle in enumerate([0, -0.4, 0, -2.4, 0, 2.0, 0.8]):\n",
    "        p.resetJointState(robot_id, i, angle)\n",
    "    \n",
    "    left_zone, right_zone = {'x': (0.1, 0.3), 'y': (0.1, 0.3)}, {'x': (0.1, 0.3), 'y': (-0.3, -0.1)}\n",
    "    # Randomly assign cube and tray zones to ensure diversity.\n",
    "    cube_zone, tray_zone = (left_zone, right_zone) if np.random.rand() > 0.5 else (right_zone, left_zone)\n",
    "    # Randomize initial cube position within the defined zone.\n",
    "    initial_cube_pos = [np.random.uniform(*cube_zone['x']), np.random.uniform(*cube_zone['y']), 0.7]\n",
    "    cube_id = p.loadURDF(\"cube_small.urdf\", basePosition=initial_cube_pos)\n",
    "    # Randomize cube color for diversity.\n",
    "    p.changeVisualShape(cube_id, -1, rgbaColor=[np.random.uniform(0.2, 0.8), np.random.uniform(0.2, 0.8), np.random.uniform(0.2, 0.8), 1.0])\n",
    "\n",
    "    # Let the cube settle before getting its stable position.\n",
    "    for _ in range(50):\n",
    "        p.stepSimulation()\n",
    "\n",
    "    stable_cube_pos, _ = p.getBasePositionAndOrientation(cube_id)\n",
    "    # Randomize tray position within its zone.\n",
    "    tray_pos = [np.random.uniform(*tray_zone['x']), np.random.uniform(*tray_zone['y']), 0.63]\n",
    "    tray_id = p.loadURDF(\"tray/tray.urdf\", basePosition=tray_pos, globalScaling=0.6)\n",
    "    \n",
    "    lang_instruction = np.random.choice(PROMPTS) # Pick a random instruction.\n",
    "    waypoints = get_expert_waypoints(robot_id, cube_id, tray_id) # Get the expert plan\n",
    "    episode_steps = []\n",
    "    \n",
    "     # Define camera properties for capturing images.\n",
    "    view_matrix = p.computeViewMatrixFromYawPitchRoll(\n",
    "        cameraTargetPosition=[0, 0, 0.7],\n",
    "        distance=0.85,\n",
    "        yaw=50,\n",
    "        pitch=-35,\n",
    "        roll=0,\n",
    "        upAxisIndex=2\n",
    "    )\n",
    "\n",
    "    projection_matrix = p.computeProjectionMatrixFOV(fov=60, aspect=1.0, nearVal=0.01, farVal=3.0)\n",
    "\n",
    "    for i, waypoint in enumerate(waypoints):\n",
    "        # Record Observation\n",
    "        link_state = p.getLinkState(robot_id, 11, computeForwardKinematics=True)\n",
    "        current_pos, current_orn_quat = np.array(link_state[0]), np.array(link_state[1])\n",
    "        current_gripper_state = 1.0 if p.getJointState(robot_id, 9)[0] < 0.02 else 0.0\n",
    "        \n",
    "        img_data = p.getCameraImage(224, 224, view_matrix, projection_matrix)\n",
    "        image = np.reshape(img_data[2], (224, 224, 4))[:, :, :3]\n",
    "        #save image\n",
    "        # tf.keras.preprocessing.image.save_img(\"00.png\", image)\n",
    "        if waypoint['type'] == 'gripper':\n",
    "            pos_delta = np.zeros(3)\n",
    "            gripper_cmd = waypoint['cmd']\n",
    "            gripper_target = 0.00 if waypoint['cmd'] > 0.5 else 0.04\n",
    "            # Simulate for a few steps to complete the action.\n",
    "            for _ in range(30):\n",
    "                p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, gripper_target)\n",
    "                p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, gripper_target)\n",
    "                p.stepSimulation()\n",
    "        else: # 'move' action\n",
    "            # Action is the change in position.\n",
    "            pos_delta = waypoint['pos'] - current_pos\n",
    "            gripper_cmd = current_gripper_state\n",
    "            joint_poses = p.calculateInverseKinematics(robot_id, 11, waypoint['pos'], waypoint['orn'])\n",
    "            # Simulate for more steps to reach the target.\n",
    "            for _ in range(60): \n",
    "                for j in range(7):\n",
    "                    p.setJointMotorControl2(robot_id, j, p.POSITION_CONTROL, joint_poses[j], force=120)\n",
    "                p.stepSimulation()\n",
    "        # Store the complete step data\n",
    "        episode_steps.append({\n",
    "            'observation': {'image': image, 'proprio': np.concatenate([current_pos, p.getEulerFromQuaternion(current_orn_quat), [current_gripper_state]]).astype(np.float32)},\n",
    "            'action': np.concatenate([pos_delta, [0,0,0], [gripper_cmd]]).astype(np.float32),\n",
    "            'language_instruction': lang_instruction,\n",
    "            'is_first': i == 0, 'is_last': i == len(waypoints) - 1, 'is_terminal': i == len(waypoints) - 1,\n",
    "        })\n",
    "\n",
    "    p.disconnect()\n",
    "    \n",
    "    # Return data for the whole episode.\n",
    "    return f'episode_{episode_idx}', {\n",
    "        'steps': episode_steps,\n",
    "        'initial_state': {\n",
    "            'cube_pos': np.array(stable_cube_pos).astype(np.float32),\n",
    "            'tray_pos': np.array(tray_pos).astype(np.float32)\n",
    "        }\n",
    "    }\n",
    "\n",
    "class MyRobotDataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"A TFDS builder for the generated robot dataset.\"\"\"\n",
    "    VERSION = tfds.core.Version('1.0.0')\n",
    "    RELEASE_NOTES = {'1.0.0': 'Initial release for pick and place.'}\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                'steps': tfds.features.Dataset({\n",
    "                    'observation': tfds.features.FeaturesDict({'image': tfds.features.Image(shape=(224, 224, 3), dtype=tf.uint8), 'proprio': tfds.features.Tensor(shape=(7,), dtype=tf.float32)}),\n",
    "                    'action': tfds.features.Tensor(shape=(7,), dtype=tf.float32), 'language_instruction': tfds.features.Text(),\n",
    "                    'is_first': tfds.features.Scalar(dtype=tf.bool), 'is_last': tfds.features.Scalar(dtype=tf.bool), 'is_terminal': tfds.features.Scalar(dtype=tf.bool),\n",
    "                }),\n",
    "                'initial_state': tfds.features.FeaturesDict({\n",
    "                    'cube_pos': tfds.features.Tensor(shape=(3,), dtype=tf.float32),\n",
    "                    'tray_pos': tfds.features.Tensor(shape=(3,), dtype=tf.float32),\n",
    "                })\n",
    "            }),\n",
    "        )\n",
    "    def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n",
    "        return {'train': self._generate_examples()}\n",
    "\n",
    "    def _generate_examples(self):\n",
    "        \"\"\"\"Yields episodes for TFDS to write to disk.\"\"\"\n",
    "        print(f\"Generating {NUM_EPISODES} episodes for RLDS dataset...\")\n",
    "        for i in tqdm(range(NUM_EPISODES)):\n",
    "            yield generate_episode_for_rlds(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35959197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to datasets\\my_robot_dataset\\my_robot_dataset\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b646dea2134245acd9193b119204b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7622a0be2aa442e938807fa7c03c9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 episodes for RLDS dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0883a623953343d9a80763a54a125d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling datasets\\my_robot_dataset\\my_robot_dataset\\incomplete.EFJJ7Y_1.0.0\\my_robot_dataset-train.tfrecord*.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset my_robot_dataset downloaded and prepared to datasets\\my_robot_dataset\\my_robot_dataset\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\n",
      "RLDS dataset generated successfully at: datasets\\my_robot_dataset\\my_robot_dataset\\1.0.0\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.makedirs(DATASET_PATH)\n",
    "\n",
    "builder = MyRobotDataset(data_dir=os.path.join(DATASET_PATH, DATASET_NAME))\n",
    "builder.download_and_prepare()\n",
    "print(f\"\\nRLDS dataset generated successfully at: {builder.data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b0d15",
   "metadata": {},
   "source": [
    "## 4. Verifying the Dataset: Replaying an Episode\n",
    "\n",
    "This final part serves as a **validation tool**. Its purpose is to load a specific episode from the dataset we just created, and then \"replay\" the recorded actions in a new PyBullet simulation. This allows us to visually confirm that the data (initial states, observations, and actions) was saved correctly and that the recorded actions produce the expected behavior.\n",
    "\n",
    "### The Replay Process (`replay_rlds_episode`)\n",
    "The function works in four main stages:\n",
    "\n",
    "1.  **Load the Dataset**\n",
    "2.  **Select an Episode**\n",
    "3.  **Reconstruct the Scene**\n",
    "4.  **Playback Actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02241a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full path name for the generated TFDS dataset, including version.\n",
    "DATASET_NAME = \"my_robot_dataset/my_robot_dataset/1.0.0\"\n",
    "\n",
    "def replay_rlds_episode(episode_id_to_replay):\n",
    "    \"\"\"\n",
    "    Loads and replays a specific episode from the saved TFDS dataset for visual verification.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_dataset_path = os.path.join(DATASET_PATH, DATASET_NAME)\n",
    "    print(f\"Loading RLDS dataset from: {full_dataset_path}\")\n",
    "\n",
    "    # Load the dataset builder from the files on disk.\n",
    "    try:\n",
    "        builder = tfds.builder_from_directory(full_dataset_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\"); return\n",
    "    \n",
    "    # Get the 'train' split of the dataset.\n",
    "    ds = builder.as_dataset(split='train')\n",
    "    \n",
    "\n",
    "    # Efficiently find the specific episode to replay.\n",
    "    try:\n",
    "        episode_to_replay = next(iter(ds.skip(episode_id_to_replay).take(1)))\n",
    "        print(f\"Successfully found episode {episode_id_to_replay}.\")\n",
    "    except (tf.errors.OutOfRangeError, StopIteration):\n",
    "        print(f\"Error: Episode with ID {episode_id_to_replay} not found.\"); return\n",
    "\n",
    "    # Read the initial object positions from the loaded episode data.\n",
    "    initial_state = episode_to_replay['initial_state']\n",
    "    cube_pos = initial_state['cube_pos'].numpy()\n",
    "    tray_pos = initial_state['tray_pos'].numpy()\n",
    "    \n",
    "\n",
    "    p.connect(p.GUI)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    p.setGravity(0, 0, -9.81)\n",
    "    p.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)\n",
    "    p.configureDebugVisualizer(p.COV_ENABLE_SHADOWS, 1)\n",
    "    p.configureDebugVisualizer(p.COV_ENABLE_RENDERING, 1)\n",
    "\n",
    "    p.loadURDF(\"plane.urdf\")\n",
    "    p.loadURDF(\"table/table.urdf\", basePosition=[0, 0, 0])\n",
    "    robot_id = p.loadURDF(\"franka_panda/panda.urdf\", basePosition=[-0.45, 0, 0.625], useFixedBase=True)\n",
    "    for i, angle in enumerate([0, -0.4, 0, -2.4, 0, 2.0, 0.8]):\n",
    "        p.resetJointState(robot_id, i, angle)\n",
    "    \n",
    "    p.resetDebugVisualizerCamera(cameraDistance=1.2, cameraYaw=70, cameraPitch=-35, cameraTargetPosition=[0.1, 0, 0.65])\n",
    "    \n",
    "    # Place the cube and tray at their recorded initial positions.\n",
    "    cube_id = p.loadURDF(\"cube_small.urdf\", basePosition=cube_pos)\n",
    "    p.changeVisualShape(cube_id, -1, rgbaColor=[0.8, 0.2, 0.2, 1.0]) \n",
    "    tray_id = p.loadURDF(\"tray/tray.urdf\", basePosition=tray_pos, globalScaling=0.6)\n",
    "\n",
    "    print(f\"Replaying Episode {episode_id_to_replay}...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    for step in episode_to_replay['steps']:\n",
    "        # Get the recorded action for this timestep.\n",
    "        action_delta = step['action'].numpy()\n",
    "        \n",
    "        # Get the robot's current state to calculate the target state.\n",
    "        link_state = p.getLinkState(robot_id, 11, computeForwardKinematics=True)\n",
    "        current_pos, current_orn_quat = np.array(link_state[0]), np.array(link_state[1])\n",
    "        \n",
    "        pos_delta, orn_delta_rpy, gripper_cmd = action_delta[:3], action_delta[3:6], action_delta[6]\n",
    "        \n",
    "        # Calculate the target position and orientation.\n",
    "        target_pos = current_pos + pos_delta\n",
    "        \n",
    "        orn_delta_quat = p.getQuaternionFromEuler(orn_delta_rpy)\n",
    "        _, target_orn_quat = p.multiplyTransforms(current_pos, current_orn_quat, [0,0,0], orn_delta_quat)\n",
    "        \n",
    "        gripper_target = 0.00 if gripper_cmd > 0.5 else 0.04\n",
    "        \n",
    "         # Execute the action over a short duration for smooth visualization.\n",
    "        for _ in range(30):\n",
    "            joint_poses = p.calculateInverseKinematics(robot_id, 11, target_pos, target_orn_quat)\n",
    "            for i in range(7):\n",
    "                p.setJointMotorControl2(robot_id, i, p.POSITION_CONTROL, joint_poses[i], force=120)\n",
    "            p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, gripper_target, force=50)\n",
    "            p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, gripper_target, force=50)\n",
    "            p.stepSimulation()\n",
    "            time.sleep(1/240.)\n",
    "            \n",
    "    print(\"Replay finished.\")\n",
    "    time.sleep(3)\n",
    "    p.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b1ea704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RLDS dataset from: datasets\\my_robot_dataset/my_robot_dataset/1.0.0\n",
      "Successfully found episode 10.\n",
      "Replaying Episode 10...\n",
      "Replay finished.\n"
     ]
    }
   ],
   "source": [
    "replay_rlds_episode(10) # Change the episode ID as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
